{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaIndex + Qdrant RAG Pipeline\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thierrypdamiba/qdrant-etl-cookbook/blob/main/notebooks/agents/llamaindex_qdrant.ipynb)\n",
        "\n",
        "Build a RAG pipeline using LlamaIndex's document loaders, chunking, and query engine backed by Qdrant.\n",
        "\n",
        "**Requirements:** Set `OPENAI_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q llama-index llama-index-vector-stores-qdrant qdrant-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, Document, Settings\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"llamaindex_demo\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create documents (replace with your own data source)\n",
        "documents = [\n",
        "    Document(text=\"Qdrant is a vector similarity search engine and database. It provides a production-ready service with a convenient API to store, search, and manage points with payloads.\"),\n",
        "    Document(text=\"HNSW (Hierarchical Navigable Small World) is the primary index used in Qdrant. Key parameters are m (connections per node) and ef_construct (search depth during build).\"),\n",
        "    Document(text=\"Quantization reduces memory footprint. Scalar quantization (int8) gives 4x reduction. Binary quantization gives 32x reduction but works best with high-dimensional vectors.\"),\n",
        "    Document(text=\"Multi-tenancy in Qdrant uses payload-based filtering. Create a keyword index on tenant_id for fast isolation between tenants.\"),\n",
        "    Document(text=\"Hybrid search in Qdrant combines dense vector similarity with sparse BM25 keyword matching for better retrieval quality.\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build index from documents\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "print(f\"Indexed {len(documents)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query the index\n",
        "query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "response = query_engine.query(\"How can I reduce memory usage in Qdrant?\")\n",
        "print(\"Answer:\", response)\n",
        "print(\"\\nSources:\")\n",
        "for node in response.source_nodes:\n",
        "    print(f\"  Score: {node.score:.4f} | {node.text[:80]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat mode with memory\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n",
        "\n",
        "response1 = chat_engine.chat(\"What indexing does Qdrant use?\")\n",
        "print(\"Q1:\", response1)\n",
        "\n",
        "response2 = chat_engine.chat(\"What are the key parameters for it?\")\n",
        "print(\"\\nQ2 (follow-up):\", response2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.11.0" }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
