{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agentic Memory System with Qdrant\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thierrypdamiba/qdrant-etl-cookbook/blob/main/notebooks/agents/agentic_memory.ipynb)\n",
        "\n",
        "Build an agent with persistent memory that learns from conversations. Uses Qdrant to store and retrieve memories across sessions.\n",
        "\n",
        "**Requirements:** Set `OPENAI_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q qdrant-client sentence-transformers openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import uuid\n",
        "from openai import OpenAI\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import (\n",
        "    PointStruct, VectorParams, Distance,\n",
        "    Filter, FieldCondition, MatchValue,\n",
        "    PayloadSchemaType,\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qdrant = QdrantClient(\":memory:\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create memory collection with metadata indexes\n",
        "qdrant.create_collection(\n",
        "    collection_name=\"agent_memory\",\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "qdrant.create_payload_index(\n",
        "    collection_name=\"agent_memory\",\n",
        "    field_name=\"memory_type\",\n",
        "    field_schema=PayloadSchemaType.KEYWORD,\n",
        ")\n",
        "\n",
        "qdrant.create_payload_index(\n",
        "    collection_name=\"agent_memory\",\n",
        "    field_name=\"session_id\",\n",
        "    field_schema=PayloadSchemaType.KEYWORD,\n",
        ")\n",
        "\n",
        "print(\"Memory collection created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentMemory:\n",
        "    \"\"\"Persistent memory system backed by Qdrant.\"\"\"\n",
        "\n",
        "    def __init__(self, client, embedder, collection=\"agent_memory\"):\n",
        "        self.client = client\n",
        "        self.embedder = embedder\n",
        "        self.collection = collection\n",
        "\n",
        "    def store(self, text: str, memory_type: str, session_id: str, metadata: dict = None):\n",
        "        \"\"\"Store a memory with type tagging (fact, preference, conversation, etc).\"\"\"\n",
        "        vector = self.embedder.encode(text).tolist()\n",
        "        payload = {\n",
        "            \"text\": text,\n",
        "            \"memory_type\": memory_type,\n",
        "            \"session_id\": session_id,\n",
        "            \"timestamp\": time.time(),\n",
        "            **(metadata or {}),\n",
        "        }\n",
        "        point_id = str(uuid.uuid4())\n",
        "        self.client.upsert(\n",
        "            collection_name=self.collection,\n",
        "            points=[PointStruct(id=point_id, vector=vector, payload=payload)],\n",
        "        )\n",
        "        return point_id\n",
        "\n",
        "    def recall(self, query: str, memory_type: str = None, limit: int = 5):\n",
        "        \"\"\"Retrieve relevant memories, optionally filtered by type.\"\"\"\n",
        "        vector = self.embedder.encode(query).tolist()\n",
        "        query_filter = None\n",
        "        if memory_type:\n",
        "            query_filter = Filter(\n",
        "                must=[FieldCondition(key=\"memory_type\", match=MatchValue(value=memory_type))]\n",
        "            )\n",
        "        response = self.client.query_points(\n",
        "            collection_name=self.collection,\n",
        "            query=vector,\n",
        "            query_filter=query_filter,\n",
        "            limit=limit,\n",
        "        )\n",
        "        return [(p.payload[\"text\"], p.score) for p in response.points]\n",
        "\n",
        "memory = AgentMemory(qdrant, embedder)\n",
        "print(\"Memory system initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate a conversation where the agent learns\n",
        "session = \"session_001\"\n",
        "\n",
        "# Store user preferences\n",
        "memory.store(\"User prefers Python over JavaScript\", \"preference\", session)\n",
        "memory.store(\"User is building a RAG application for legal documents\", \"fact\", session)\n",
        "memory.store(\"User's Qdrant instance is on Qdrant Cloud with 2M vectors\", \"fact\", session)\n",
        "memory.store(\"User asked about quantization for memory optimization\", \"conversation\", session)\n",
        "memory.store(\"User prefers concise code examples over verbose explanations\", \"preference\", session)\n",
        "\n",
        "print(\"Stored 5 memories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agent uses memories to give personalized responses\n",
        "def agent_respond(user_query: str):\n",
        "    # Recall relevant memories\n",
        "    all_memories = memory.recall(user_query, limit=3)\n",
        "    preferences = memory.recall(user_query, memory_type=\"preference\", limit=2)\n",
        "\n",
        "    memory_context = \"\\n\".join([f\"- {text} (relevance: {score:.2f})\" for text, score in all_memories])\n",
        "    pref_context = \"\\n\".join([f\"- {text}\" for text, _ in preferences])\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"You are a helpful Qdrant assistant with memory of past interactions.\n",
        "\n",
        "Relevant memories:\n",
        "{memory_context}\n",
        "\n",
        "User preferences:\n",
        "{pref_context}\n",
        "\n",
        "Use these memories to personalize your response.\"\"\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": user_query},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    # Store this interaction as a new memory\n",
        "    memory.store(f\"User asked: {user_query}\", \"conversation\", session)\n",
        "\n",
        "    return answer, all_memories\n",
        "\n",
        "answer, used_memories = agent_respond(\"How should I optimize my setup?\")\n",
        "print(\"Answer:\", answer)\n",
        "print(\"\\nMemories used:\")\n",
        "for text, score in used_memories:\n",
        "    print(f\"  [{score:.2f}] {text}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.11.0" }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
