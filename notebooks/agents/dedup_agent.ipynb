{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deduplication Agent\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thierrypdamiba/qdrant-etl-cookbook/blob/main/notebooks/agents/dedup_agent.ipynb)\n",
        "\n",
        "Finds and removes near-duplicate entries in a Qdrant collection using vector similarity thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q qdrant-client sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, PointIdsList, VectorParams, Distance\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(\":memory:\")\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create collection with intentional duplicates\n",
        "client.create_collection(\n",
        "    collection_name=\"with_dupes\",\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "texts = [\n",
        "    \"Qdrant is a vector database\",\n",
        "    \"Qdrant is a vector database engine\",  # near-duplicate of 0\n",
        "    \"Python is a programming language\",\n",
        "    \"Python is a popular programming language\",  # near-duplicate of 2\n",
        "    \"Machine learning uses neural networks\",\n",
        "]\n",
        "\n",
        "points = [\n",
        "    PointStruct(id=i, vector=model.encode(t).tolist(), payload={\"text\": t})\n",
        "    for i, t in enumerate(texts)\n",
        "]\n",
        "client.upsert(collection_name=\"with_dupes\", points=points)\n",
        "print(f\"Loaded {len(points)} points (with intentional near-duplicates)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_duplicates(collection: str, threshold: float = 0.95):\n",
        "    \"\"\"Scan collection and find near-duplicate pairs.\"\"\"\n",
        "    duplicates = []\n",
        "    offset = None\n",
        "\n",
        "    while True:\n",
        "        records, offset = client.scroll(\n",
        "            collection_name=collection,\n",
        "            limit=100,\n",
        "            offset=offset,\n",
        "            with_vectors=True,\n",
        "        )\n",
        "\n",
        "        if not records:\n",
        "            break\n",
        "\n",
        "        for record in records:\n",
        "            similar_response = client.query_points(\n",
        "                collection_name=collection,\n",
        "                query=record.vector,\n",
        "                limit=5,\n",
        "                score_threshold=threshold,\n",
        "            )\n",
        "            similar = similar_response.points\n",
        "\n",
        "            for match in similar:\n",
        "                if match.id != record.id and match.id > record.id:\n",
        "                    duplicates.append((record.id, match.id, match.score))\n",
        "\n",
        "        if offset is None:\n",
        "            break\n",
        "\n",
        "    return duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dupes = find_duplicates(\"with_dupes\", threshold=0.90)\n",
        "\n",
        "print(f\"Found {len(dupes)} duplicate pairs:\")\n",
        "for orig_id, dup_id, score in dupes:\n",
        "    print(f\"  ID {orig_id} <-> ID {dup_id} (similarity: {score:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove duplicates (keep the lower ID)\n",
        "ids_to_remove = list(set(dup_id for _, dup_id, _ in dupes))\n",
        "\n",
        "if ids_to_remove:\n",
        "    client.delete(\n",
        "        collection_name=\"with_dupes\",\n",
        "        points_selector=PointIdsList(points=ids_to_remove),\n",
        "    )\n",
        "    print(f\"Removed {len(ids_to_remove)} duplicate points\")\n",
        "\n",
        "# Verify\n",
        "remaining, _ = client.scroll(collection_name=\"with_dupes\", limit=100)\n",
        "print(f\"Remaining points: {len(remaining)}\")\n",
        "for r in remaining:\n",
        "    print(f\"  ID {r.id}: {r.payload['text']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}