{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain + Qdrant Conversational RAG\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thierrypdamiba/qdrant-etl-cookbook/blob/main/notebooks/agents/langchain_qdrant.ipynb)\n",
        "\n",
        "Build a conversational RAG chain with LangChain, using Qdrant as the vector store and OpenAI for generation.\n",
        "\n",
        "**Requirements:** Set `OPENAI_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-openai langchain-qdrant qdrant-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from qdrant_client import QdrantClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(\":memory:\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create documents\n",
        "docs = [\n",
        "    Document(page_content=\"Qdrant stores vectors with associated payloads. Payloads can contain any JSON data and can be indexed for fast filtering.\", metadata={\"source\": \"docs\"}),\n",
        "    Document(page_content=\"HNSW index parameters: m controls the number of bi-directional links, ef_construct controls index build quality. Higher values mean better recall but more memory.\", metadata={\"source\": \"docs\"}),\n",
        "    Document(page_content=\"Scalar quantization converts float32 to int8, reducing memory by 4x. Use oversampling=2.0 and rescore=true for minimal accuracy loss.\", metadata={\"source\": \"docs\"}),\n",
        "    Document(page_content=\"Create payload indexes on fields you filter by. Supported types: keyword (exact match), integer (range), float (range), geo (radius), text (full-text).\", metadata={\"source\": \"docs\"}),\n",
        "    Document(page_content=\"For multi-tenancy, use a tenant_id payload field with a keyword index. All searches should filter by tenant_id to ensure data isolation.\", metadata={\"source\": \"docs\"}),\n",
        "]\n",
        "\n",
        "# Build Qdrant vector store\n",
        "vector_store = QdrantVectorStore.from_documents(\n",
        "    docs,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"langchain_demo\",\n",
        ")\n",
        "print(f\"Indexed {len(docs)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build retrieval chain\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the question using the provided context. If the context doesn't have the answer, say so.\\n\\nContext:\\n{context}\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask questions\n",
        "response = rag_chain.invoke({\"input\": \"How do I optimize memory usage in Qdrant?\"})\n",
        "print(\"Answer:\", response[\"answer\"])\n",
        "print(f\"\\nUsed {len(response['context'])} source documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = rag_chain.invoke({\"input\": \"How should I set up multi-tenancy?\"})\n",
        "print(\"Answer:\", response[\"answer\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.11.0" }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
