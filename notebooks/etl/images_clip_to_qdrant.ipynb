{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Embeddings with CLIP\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thierrypdamiba/qdrant-etl-cookbook/blob/main/notebooks/etl/images_clip_to_qdrant.ipynb)\n",
        "\n",
        "Generate image embeddings using OpenAI CLIP and store them in Qdrant for visual similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q qdrant-client transformers pillow torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(\":memory:\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.create_collection(\n",
        "    collection_name=\"images\",\n",
        "    vectors_config=VectorParams(size=512, distance=Distance.COSINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic images for demo (colored squares)\n",
        "colors = {\n",
        "    \"red\": (255, 0, 0),\n",
        "    \"green\": (0, 255, 0),\n",
        "    \"blue\": (0, 0, 255),\n",
        "    \"yellow\": (255, 255, 0),\n",
        "    \"purple\": (128, 0, 128),\n",
        "}\n",
        "\n",
        "points = []\n",
        "for idx, (name, rgb) in enumerate(colors.items()):\n",
        "    img = Image.new(\"RGB\", (224, 224), rgb)\n",
        "    inputs = processor(images=img, return_tensors=\"pt\")\n",
        "    embedding = model.get_image_features(**inputs).detach().numpy()[0].tolist()\n",
        "\n",
        "    points.append(\n",
        "        PointStruct(\n",
        "            id=idx,\n",
        "            vector=embedding,\n",
        "            payload={\"color\": name, \"rgb\": list(rgb)},\n",
        "        )\n",
        "    )\n",
        "\n",
        "client.upsert(collection_name=\"images\", points=points)\n",
        "print(f\"Indexed {len(points)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search with text query (CLIP cross-modal search)\n",
        "text_inputs = processor(text=[\"a warm sunset color\"], return_tensors=\"pt\")\n",
        "text_embedding = model.get_text_features(**text_inputs).detach().numpy()[0].tolist()\n",
        "\n",
        "results = client.search(\n",
        "    collection_name=\"images\",\n",
        "    query_vector=text_embedding,\n",
        "    limit=3,\n",
        ")\n",
        "\n",
        "print(\"Query: 'a warm sunset color'\")\n",
        "for r in results:\n",
        "    print(f\"  Score: {r.score:.4f} | {r.payload['color']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
