{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Search: Text + Images with CLIP\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thierrypdamiba/qdrant-etl-cookbook/blob/main/notebooks/etl/multimodal_search.ipynb)\n",
        "\n",
        "Build a cross-modal search system where you can search images with text and text with images using CLIP embeddings stored in Qdrant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q qdrant-client transformers pillow torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(\":memory:\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Both text and images live in the same vector space with CLIP\n",
        "client.create_collection(\n",
        "    collection_name=\"multimodal\",\n",
        "    vectors_config=VectorParams(size=512, distance=Distance.COSINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Index some images (synthetic colored squares for demo)\n",
        "image_items = [\n",
        "    {\"name\": \"sunset\", \"color\": (255, 100, 50)},\n",
        "    {\"name\": \"ocean\", \"color\": (30, 100, 200)},\n",
        "    {\"name\": \"forest\", \"color\": (34, 139, 34)},\n",
        "    {\"name\": \"snow\", \"color\": (240, 240, 255)},\n",
        "    {\"name\": \"night\", \"color\": (20, 20, 40)},\n",
        "]\n",
        "\n",
        "# Index some text documents too\n",
        "text_items = [\n",
        "    \"A beautiful sunset over the mountains with orange and red hues\",\n",
        "    \"Deep blue ocean waves crashing on a sandy beach\",\n",
        "    \"Dense green forest with tall pine trees and moss\",\n",
        "    \"Fresh white snow covering a quiet village in winter\",\n",
        "    \"Starry night sky with the milky way visible\",\n",
        "]\n",
        "\n",
        "points = []\n",
        "idx = 0\n",
        "\n",
        "# Embed images\n",
        "for item in image_items:\n",
        "    img = Image.new(\"RGB\", (224, 224), item[\"color\"])\n",
        "    inputs = processor(images=img, return_tensors=\"pt\")\n",
        "    emb = model.get_image_features(**inputs).detach().numpy()[0].tolist()\n",
        "    points.append(PointStruct(\n",
        "        id=idx, vector=emb,\n",
        "        payload={\"type\": \"image\", \"name\": item[\"name\"], \"color\": list(item[\"color\"])},\n",
        "    ))\n",
        "    idx += 1\n",
        "\n",
        "# Embed text\n",
        "for text in text_items:\n",
        "    inputs = processor(text=[text], return_tensors=\"pt\")\n",
        "    emb = model.get_text_features(**inputs).detach().numpy()[0].tolist()\n",
        "    points.append(PointStruct(\n",
        "        id=idx, vector=emb,\n",
        "        payload={\"type\": \"text\", \"content\": text},\n",
        "    ))\n",
        "    idx += 1\n",
        "\n",
        "client.upsert(collection_name=\"multimodal\", points=points)\n",
        "print(f\"Indexed {len(image_items)} images + {len(text_items)} texts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text-to-anything search (finds both matching images and text)\n",
        "query_text = \"warm colors like fire\"\n",
        "inputs = processor(text=[query_text], return_tensors=\"pt\")\n",
        "query_vec = model.get_text_features(**inputs).detach().numpy()[0].tolist()\n",
        "\n",
        "response = client.query_points(\n",
        "    collection_name=\"multimodal\",\n",
        "    query=query_vec,\n",
        "    limit=5,\n",
        ")\n",
        "results = response.points\n",
        "\n",
        "print(f\"Query: '{query_text}'\")\n",
        "for r in results:\n",
        "    if r.payload[\"type\"] == \"image\":\n",
        "        print(f\"  Score: {r.score:.4f} | [IMAGE] {r.payload['name']}\")\n",
        "    else:\n",
        "        print(f\"  Score: {r.score:.4f} | [TEXT] {r.payload['content'][:60]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image-to-anything search\n",
        "query_img = Image.new(\"RGB\", (224, 224), (0, 50, 150))  # dark blue\n",
        "inputs = processor(images=query_img, return_tensors=\"pt\")\n",
        "query_vec = model.get_image_features(**inputs).detach().numpy()[0].tolist()\n",
        "\n",
        "response = client.query_points(\n",
        "    collection_name=\"multimodal\",\n",
        "    query=query_vec,\n",
        "    limit=5,\n",
        ")\n",
        "results = response.points\n",
        "\n",
        "print(\"Query: [dark blue image]\")\n",
        "for r in results:\n",
        "    if r.payload[\"type\"] == \"image\":\n",
        "        print(f\"  Score: {r.score:.4f} | [IMAGE] {r.payload['name']}\")\n",
        "    else:\n",
        "        print(f\"  Score: {r.score:.4f} | [TEXT] {r.payload['content'][:60]}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.11.0" }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
