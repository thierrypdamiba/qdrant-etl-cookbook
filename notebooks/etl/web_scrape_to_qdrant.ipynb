{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Scraping to Qdrant\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thierrypdamiba/qdrant-etl-cookbook/blob/main/notebooks/etl/web_scrape_to_qdrant.ipynb)\n",
        "\n",
        "Scrape web pages with BeautifulSoup, clean and chunk HTML content, then load into Qdrant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q qdrant-client sentence-transformers beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(\":memory:\")\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.create_collection(\n",
        "    collection_name=\"web_pages\",\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://qdrant.tech/documentation/overview/\",\n",
        "]\n",
        "\n",
        "points = []\n",
        "for idx, url in enumerate(urls):\n",
        "    resp = requests.get(url, timeout=10)\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # Remove non-content elements\n",
        "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    text = soup.get_text(separator=\" \", strip=True)[:2000]\n",
        "    title = soup.title.string if soup.title else url\n",
        "    embedding = model.encode(text).tolist()\n",
        "\n",
        "    points.append(\n",
        "        PointStruct(\n",
        "            id=idx,\n",
        "            vector=embedding,\n",
        "            payload={\"url\": url, \"text\": text, \"title\": title},\n",
        "        )\n",
        "    )\n",
        "    print(f\"Scraped: {title}\")\n",
        "\n",
        "client.upsert(collection_name=\"web_pages\", points=points)\n",
        "print(f\"\\nLoaded {len(points)} pages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search\n",
        "query_vector = model.encode(\"What is Qdrant?\").tolist()\n",
        "response = client.query_points(\n",
        "    collection_name=\"web_pages\",\n",
        "    query=query_vector,\n",
        "    limit=3,\n",
        ")\n",
        "results = response.points\n",
        "\n",
        "for r in results:\n",
        "    print(f\"Score: {r.score:.4f} | {r.payload['title']}\")\n",
        "    print(f\"  {r.payload['text'][:150]}...\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}