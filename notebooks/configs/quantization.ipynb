{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantization Settings\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thierrypdamiba/qdrant-etl-cookbook/blob/main/notebooks/configs/quantization.ipynb)\n",
        "\n",
        "Reduce memory usage with scalar, binary, and product quantization. Trade-offs between memory, speed, and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q qdrant-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import (\n",
        "    VectorParams,\n",
        "    Distance,\n",
        "    ScalarQuantization,\n",
        "    ScalarQuantizationConfig,\n",
        "    ScalarType,\n",
        "    BinaryQuantization,\n",
        "    BinaryQuantizationConfig,\n",
        "    ProductQuantization,\n",
        "    ProductQuantizationConfig,\n",
        "    CompressionRatio,\n",
        "    SearchParams,\n",
        "    QuantizationSearchParams,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = QdrantClient(\":memory:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scalar quantization: 4x memory reduction, minimal accuracy loss\n",
        "client.create_collection(\n",
        "    collection_name=\"scalar_quantized\",\n",
        "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    quantization_config=ScalarQuantization(\n",
        "        scalar=ScalarQuantizationConfig(\n",
        "            type=ScalarType.INT8,\n",
        "            quantile=0.99,\n",
        "            always_ram=True,\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "info = client.get_collection(\"scalar_quantized\")\n",
        "print(f\"Scalar quantized collection created\")\n",
        "print(f\"  Quantization: {info.config.quantization_config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary quantization: 32x memory reduction, works best with high-dim vectors\n",
        "client.create_collection(\n",
        "    collection_name=\"binary_quantized\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        "    quantization_config=BinaryQuantization(\n",
        "        binary=BinaryQuantizationConfig(\n",
        "            always_ram=True,\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "print(\"Binary quantized collection created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Product quantization: configurable compression ratio\n",
        "client.create_collection(\n",
        "    collection_name=\"product_quantized\",\n",
        "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    quantization_config=ProductQuantization(\n",
        "        product=ProductQuantizationConfig(\n",
        "            compression=CompressionRatio.X16,\n",
        "            always_ram=True,\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "print(\"Product quantized collection created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search with quantization oversampling for better accuracy\n",
        "# (would work with actual data in the collection)\n",
        "search_params = SearchParams(\n",
        "    quantization=QuantizationSearchParams(\n",
        "        ignore=False,\n",
        "        rescore=True,\n",
        "        oversampling=2.0,  # fetch 2x candidates, rescore with full vectors\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Quantization search params configured:\")\n",
        "print(f\"  rescore={search_params.quantization.rescore}\")\n",
        "print(f\"  oversampling={search_params.quantization.oversampling}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
